---
layout: post
title: Intel-Crick NGS workshop
comments: true
excerpt_separator: <!--more-->
---

_Originally posted on the 7th of October 2013_

Early in September I had the opportunity to attend the inaugural [Crick-Intel Workshop](http://www3.imperial.ac.uk/newsandeventspggrp/imperialcollege/eventssummary/event_21-6-2013-12-4-35) on “Computational challenges and performance optimizations in NGS data analyses”.  This course was designed jointly by Intel and the Crick institute to address performance issues relating to the analysis of Next Generation Sequencing (NGS) data and train the participants in the practicalities of optimising NGS analysis pipelines.
<!--more-->
I should stress that the course was actually held in the Darwin room of the [Wellcome Trust HQ](http://www.wellcome.ac.uk/About-us/Contact-us/Our-headquarters/index.htm) (a very impressive building) since the Crick institute has yet to be built.  The [Crick itself](https://www.crick.ac.uk/about-us/) is coming out of a merger of the [MRC National Institute for Medical Research (NIMR)](http://www.nimr.mrc.ac.uk/) and [Cancer Research UK’s London Research Institute](http://www.london-research-institute.org.uk/) along with the [Wellcome Trust](http://www.wellcome.ac.uk/), [UCL](http://www.ucl.ac.uk/), [Imperial](http://www.imperial.ac.uk/) and [Kings College London](http://www.kcl.ac.uk/).  Currently building is underway behind the British Library and the results are expected to be [very impressive](https://www.crick.ac.uk/the-new-building/architecture/).  Interestingly once completed the institute hopes to house over 200 Bioinformaticians amongst 1000s of others in the life science area – which should have interesting implications for the future Bioinformatics jobs market.  I should also stress what follows is my synthesis of the meeting not a word for word account and my take home on the proceedings and in no way reflects any official university line on such matters.

There where various speakers both from Intel and the Crick institute itself.  First up after introductions was [Chris Dagdigian](http://www.slideshare.net/chrisdag) of [BioTeam](http://bioteam.net/) – who appears to be THE go to person for all Pharma and life science compute needs.  Chris gave two very impressive talks during the meeting which would be fun to witness for any self confessed hardware geek, myself included.  He detailed institutional infrastructure issues generated by NGS data, namely lack of space, power and cooling needed for clusters and data storage in addition to adequate networking – all of which are hard to fix in the order of years, often requiring new building as opposed to acquiring new instrumentation (NGS hardware) to generate the data which is easy and comparatively cheep.  Also acquiring more compute is far easier than acquiring the infrastructure needed to house it.  Forward planning in building projects for power, space cooling and networking is key here.

The theme of low cost to generate data was really stressed here, in so much that for many NGS applications where a non-rare sample was used it is better to have a data deletion policy, rather than a retention one, since its cheaper to keep the sample in a -80° C freezer and re-sequence that, if required, than it is to store the data generated by the sequencing for the next 10 years or so.

Chris then went on to talk about common bottlenecks in analysis.  The biggest and most difficult of these to tackle is local IT who often have no idea how to supporting scientific computing.  Often local IT has next to zero Linux skills having spent years locking down Windows boxes for office workers.  Regarding computation it is impossible to remove all bottlenecks you will only move them elsewhere, but with careful analysis of the problem you can easily optimise your IT / cluster design to make sure it matches up with your application.  Specifically Chris stressed the importance and utility of the Fat Node, _i_._e_. a large 4U cluster node with lots of RAM rather than a greater number of smaller nodes with lower amounts of RAM.  This is essential for Bioinformatics applications as there is a need for ~8GB of RAM per execution thread especially for read alignment to the human genome.  This was nice to hear as it verified our own design considerations in speccing own hardware (Monolith) and the forthcoming cluster the Medical School is acquiring here at Newcastle.  Essentially most performance is IO bound, not CPU bound, having a local disk to CPU ratio of 1:1 is desiable here – again the Fat node wins the day.  Additionally, 10 Gigabit a second Ethernet (10 GbE) is also vitally important so your file servers can keep the analysis nodes pumped with data.  [Message Passing Interface (MPI)](https://en.wikipedia.org/wiki/Message_Passing_Interface) is only really useful for tasks like molecular dynamics or climate modeling the majority of sequenced based Bioinformatics will not be able to take advantage of MPI.  In addition most current MPI implementations in Bioinformatics are an embarrassment.  Consequently fancy [InfiniBand](https://en.wikipedia.org/wiki/InfiniBand) solutions (which focus on reducing MPI latency such as SGI UV) are overkill for NGS applications and represent poor value vs. commodity server hardware with 10 GbE.  Additionally MPI appears to be losing ground with respect to newer approaches such as [Hadoop](https://en.wikipedia.org/wiki/Apache_Hadoop) and [MapReduce](https://en.wikipedia.org/wiki/MapReduce) – although neither of these two are easy to implement.

The movement of data is key to efficiency of NGS analysis.  If your instrumentation is local you might not want to house your compute at a remote site if you can help it.  If you still need to move your data, consider moving your users to the data via a [SSH](https://en.wikipedia.org/wiki/Secure_Shell) login to the server rather than moving the data to the users via the network.  And finally when you do need to get data off your instrument, QC it first to see if its any good before you waste time sending it over the network to your analysis pipeline, only then to find out at the end it was worthless.

[Clay Breshears](https://software.intel.com/en-us/user/334588) from Intel gave a number of really insightful talks during the workshop, the first of these was an introduction to Parallelism; key concepts and terms where introduced.  Essentially there are four levels of parallelism i) instruction level, _i_._e_. [out-of-order execution](https://en.wikipedia.org/wiki/Out-of-order_execution) and [branch prediction](https://en.wikipedia.org/wiki/Branch_predictor). ii) Data level: [SIMD](https://en.wikipedia.org/wiki/SIMD), [vector units](https://en.wikipedia.org/wiki/Vector_processor).  iii) Thread level, where the programmer has to do the work of spreading compute across cores iv) Process level, _i_._e_. sharing jobs across a network _i_._e_. [Load Sharing Facility (LSF)](https://en.wikipedia.org/wiki/Platform_LSF) / [Grid Engine](https://arc.liv.ac.uk/trac/SGE).

These concepts where built upon in later talks, Clay introduced the classic [Von Neumann Architecture](https://en.wikipedia.org/wiki/Von_Neumann_architecture) paradigm of Input -> (CPU <-> Memory) -> Output.  Testing by Intel has shown that [Hyper-Threading](https://en.wikipedia.org/wiki/Hyper-threading) benefits NGS analysis (again reconfirming our own results experimenting with Monolith) in that taking advantage of Hyper-Threading on your alignment jobs by running 32 threads on your 16 CPU core server will give you a 20-30% performance increase.  Some very interesting concepts in [SIMD](https://en.wikipedia.org/wiki/SIMD) and [vectorization](https://en.wikipedia.org/wiki/Vector_processor) where explored.  Performance analysis tools such as the venerable [top](http://manpages.ubuntu.com/manpages/raring/en/man1/top.1.html), [sar](http://manpages.ubuntu.com/manpages/raring/man1/sar.sysstat.1.html) and [pidstat](http://manpages.ubuntu.com/manpages/raring/en/man1/pidstat.1.html) where also discussed. Later Clay discussed specifics of thread and process level optimisations for multi-thread computation.  The unsung hero here was Intel which has analysed the code of popular Bioinformatics tools and worked with developers to increase execution speed, specifically optimisations in [Bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml), [SOAPdenovo](https://sourceforge.net/projects/soapdenovo2/) and [ABySS](http://www.bcgsc.ca/platform/bioinfo/software/abyss) (which Clay himself worked on) were detailed.  I’m quite surprised by the lack of a prominent acknowledgement of Intel’s input here in the release notes of these tools.

There were further talks by [Ernest Turro](http://platelets.group.cam.ac.uk/people/ernest-turro) on RNA-Seq mapping strategies, [Vincent Plagnol](http://www.ucl.ac.uk/ukpdc/research-groups/neurogenetics-group/vincent-plagnol) on the details of RNA sequencing expression level optimisations – Vincent’s opinion was that RNA-Seq work on non-model organisms (without a good reference genome) should simply not be carried out!  [Eilidh Troup](https://www.epcc.ed.ac.uk/about/staff/mrs-eilidh-troup) from the [SRPINT](http://www.ed.ac.uk/pathway-medicine/our-research/dpmgroups/ghazal-group/pathway-informatics/sprint) team gave and overview of their work producing packages for parallelising R code.  [Nuno Fonseca](https://www.ebi.ac.uk/about/people/nuno-fonseca) from [Alvis Brazma’s](https://www.ebi.ac.uk/about/people/alvis-brazma) research group at the EBI detailed a comparison of RNA-Seq mapping and quantification methods (tl;dr OSA and HTSeq looks like the winner here) and Steve Searle (join head of [Ensembl](http://www.ensembl.org/index.html) along with Vertebrate annotation at the Sanger) gave an interesting talk on optimisations used in pipelines at the EBI and Sanger, the importance of C code to replace interpreted scripting languages was key to improvements here as was the use of grouping [LSF](https://en.wikipedia.org/wiki/Platform_LSF) jobs into job arrays and in a similar vein grouping DB queries into larger sets and letting the client do the separation rather than loading the DB server with lots of smaller queries.

In the final talk [Ketan Paranjape](http://iianalytics.com/about/faculty/ketan-paranjape) raised an interesting point, since clinical data can’t be stored on the cloud (owing to data protection restrictions) and that given local space for IT is limited, a good solution is for all enterprise IT: email, HR, finance and other uninteresting commodity computing be offloaded to the cloud and that local space, which is at a premium, be used for clinically relevant data and analysis.

Overall the meeting was very informative and gave those attending a good insight and overview of both hardware, infrastructure, and software routes to optimise analysis pipelines for the ensuing NGS data deluge.  I shall undoubtedly be implementing some of the ideas covered in an analysis pipeline soon!
